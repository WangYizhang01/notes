{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d310944",
   "metadata": {},
   "source": [
    "### 预备知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a1aa0",
   "metadata": {},
   "source": [
    "#### 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d29d7",
   "metadata": {},
   "source": [
    "##### 2.1 创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5118f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2acc3ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.6894e+27, 7.9463e+08, 3.2604e-12],\n",
       "        [1.7743e+28, 2.0535e-19, 5.9682e-02],\n",
       "        [7.0374e+22, 3.8946e+21, 4.4650e+30],\n",
       "        [7.0975e+22, 7.9309e+34, 7.9439e+08],\n",
       "        [3.2604e-12, 7.3113e+34, 2.0706e-19]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceed06f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1687, 0.9754, 0.8769],\n",
       "        [0.1098, 0.0990, 0.1993],\n",
       "        [0.0335, 0.2908, 0.5057],\n",
       "        [0.5457, 0.7826, 0.7080],\n",
       "        [0.4042, 0.4293, 0.5127]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54f7758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5,3,dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933b3f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5,3],dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3752502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-1.5320, -0.0186,  2.9297],\n",
      "        [ 0.9589, -0.1601, -0.3159],\n",
      "        [ 0.0076, -0.1867, -2.1629],\n",
      "        [-0.5118, -1.2420,  0.5091],\n",
      "        [ 0.0688,  0.8951, -1.1266]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54b2190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed9d60",
   "metadata": {},
   "source": [
    "##### 2.2 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecdf21",
   "metadata": {},
   "source": [
    "加法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39edd844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2957,  0.9340,  3.2519],\n",
       "        [ 1.8556, -0.0193,  0.6420],\n",
       "        [ 0.2661,  0.7964, -1.4619],\n",
       "        [ 0.0874, -0.5633,  1.3453],\n",
       "        [ 0.4716,  1.5962, -0.2711]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ee75af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2957,  0.9340,  3.2519],\n",
       "        [ 1.8556, -0.0193,  0.6420],\n",
       "        [ 0.2661,  0.7964, -1.4619],\n",
       "        [ 0.0874, -0.5633,  1.3453],\n",
       "        [ 0.4716,  1.5962, -0.2711]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b704e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2957,  0.9340,  3.2519],\n",
      "        [ 1.8556, -0.0193,  0.6420],\n",
      "        [ 0.2661,  0.7964, -1.4619],\n",
      "        [ 0.0874, -0.5633,  1.3453],\n",
      "        [ 0.4716,  1.5962, -0.2711]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0847feb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2957,  0.9340,  3.2519],\n",
       "        [ 1.8556, -0.0193,  0.6420],\n",
       "        [ 0.2661,  0.7964, -1.4619],\n",
       "        [ 0.0874, -0.5633,  1.3453],\n",
       "        [ 0.4716,  1.5962, -0.2711]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c166e",
   "metadata": {},
   "source": [
    "#### 索引    \n",
    "注意：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0341414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5320,  0.9814,  3.9297])\n",
      "tensor([-0.5320,  0.9814,  3.9297])\n"
     ]
    }
   ],
   "source": [
    "y = x[0, :]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0, :]) # 源tensor也被改了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de1e30",
   "metadata": {},
   "source": [
    "#### 改变形状    \n",
    "用view( )来改变Tensor的形状：    \n",
    "注意view( )返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52234a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0dd7bb",
   "metadata": {},
   "source": [
    "如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0949fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5320, -0.0186,  2.9297],\n",
      "        [-0.0411, -1.1601, -1.3159],\n",
      "        [-0.9924, -1.1867, -3.1629],\n",
      "        [-1.5118, -2.2420, -0.4909],\n",
      "        [-0.9312, -0.1049, -2.1266]])\n",
      "tensor([-0.5320,  0.9814,  3.9297,  0.9589, -0.1601, -0.3159,  0.0076, -0.1867,\n",
      "        -2.1629, -0.5118, -1.2420,  0.5091,  0.0688,  0.8951, -1.1266])\n"
     ]
    }
   ],
   "source": [
    "# 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor\n",
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd96f8",
   "metadata": {},
   "source": [
    "另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebe37e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9127])\n",
      "0.9126622080802917\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9327e8",
   "metadata": {},
   "source": [
    "##### 2.3 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f68a43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b30ceeb",
   "metadata": {},
   "source": [
    "##### 2.4 运算的内存开销"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98def76a",
   "metadata": {},
   "source": [
    "索引操作是不会开辟新内存的，    \n",
    "而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。    \n",
    "为了演示这一点，我们可以使用Python自带的id函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2816df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before) # False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95408be9",
   "metadata": {},
   "source": [
    "如果想指定结果到原来的y的内存，我们可以使用前面介绍的索引来进行替换操作。    \n",
    "在下面的例子中，我们把x + y的结果通过[:]写进y对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd4f14f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f3da8b",
   "metadata": {},
   "source": [
    "还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果，    \n",
    "例如torch.add(x, y, out=y)和y += x(y.add_(x))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "604e1de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06de63",
   "metadata": {},
   "source": [
    "注：虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor    \n",
    "（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa1a65",
   "metadata": {},
   "source": [
    "##### 2.5 Tensor和NumPy相互转换    \n",
    "我们很容易用numpy( )和from_numpy( )将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！    \n",
    "还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor( ), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9717e40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor转Numpy\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c711ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Numpy转Tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04241cd",
   "metadata": {},
   "source": [
    "##### 2.6 Tensor on GPU    \n",
    "用方法to( )可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a8c74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced220e",
   "metadata": {},
   "source": [
    "### 自动求梯度    \n",
    "PyTorch提供的autograd包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。本节将介绍如何使用autograd包来进行自动求梯度的有关操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08214441",
   "metadata": {},
   "source": [
    "##### 3.1 概念    \n",
    "上一节介绍的Tensor是这个包的核心类，如果将其属性.requires_grad设置为True，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用.backward( )来完成所有梯度计算。此Tensor的梯度将累积到.grad属性中。    \n",
    "如果不想要被继续追踪，可以调用.detach( )将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用with torch.no_grad( )将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（requires_grad=True）的梯度。    \n",
    "Function是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373268a",
   "metadata": {},
   "source": [
    "##### 3.2 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bac308de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 创建一个Tensor并设置requires_grad=True\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a3d048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x11f1635e0>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32717a9",
   "metadata": {},
   "source": [
    "注意x是直接创建的，所以它没有grad_fn, 而y是通过一个加法操作创建的，所以它有一个为<AddBackward>的grad_fn。    \n",
    "像x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c6828bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf, y.is_leaf) # True False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5473e1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138d0bc",
   "metadata": {},
   "source": [
    "通过.requires_grad_( )来用in-place的方式改变requires_grad属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1529b87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x11f163fd0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33baab",
   "metadata": {},
   "source": [
    "##### 3.3 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d8b3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为out是一个标量，所以调用backward()时不需要指定求导变量：\n",
    "out.backward() # 等价于 out.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e43f906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# out关于x的梯度 d(out)/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02877a",
   "metadata": {},
   "source": [
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6379a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd49419",
   "metadata": {},
   "source": [
    "现在我们解释3.1节留下的问题，为什么在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor?     \n",
    "简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。    \n",
    "举个例子，假设形状为 m x n 的矩阵 X 经过运算得到了 p x q 的矩阵 Y，Y 又经过运算得到了 s x t 的矩阵 Z。那么按照前面讲的规则，dZ/dY 应该是一个 s x t x p x q 四维张量，dY/dX 是一个 p x q x m x n的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉……     \n",
    "为了避免这个问题，我们不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量。所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量，举个例子，假设y由自变量x计算而来，w是和y同形的张量，则y.backward(w)的含义是：先计算l = torch.sum(y * w)，则l是个标量，然后求l对自变量x的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9fd1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a540763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79112be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
